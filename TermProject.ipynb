{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80151b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sweth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sweth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Text, Iterable, Union\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ff5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.wordnet_lem = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess(self, TEXT: str) -> str:\n",
    "        cleaned_text = self._clean(TEXT)\n",
    "        text_no_stop_words = self.remove_stop_words(cleaned_text)\n",
    "        text_no_freq_words = self.remove_freq_words(text_no_stop_words)\n",
    "        lemmatized_text = self.lemmatize_text(text_no_freq_words)\n",
    "        return lemmatized_text\n",
    "\n",
    "    def preprocess_dataframe(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "        preprocessed_df = df.copy()\n",
    "        preprocessed_df[text_column] = preprocessed_df[text_column].apply(self.preprocess)\n",
    "        return preprocessed_df\n",
    "\n",
    "    def _clean(self, TEXT: str) -> str:\n",
    "      \n",
    "        if not isinstance(TEXT, str):\n",
    "            TEXT = str(TEXT)\n",
    "        # converting to lowercase, removing URL links, special characters, punctuations...\n",
    "            TEXT = TEXT.lower() # converting to lowercase\n",
    "            TEXT = re.sub('https?://\\S+|www\\.\\S+', '', TEXT) # removing URL links\n",
    "            TEXT = re.sub(r\"\\b\\d+\\b\", \"\", TEXT) # removing number \n",
    "            TEXT = re.sub('<.*?>+', '', TEXT) # removing special characters, \n",
    "            TEXT = re.sub('[%s]' % re.escape(string.punctuation), '', TEXT) # punctuations\n",
    "            TEXT = re.sub('\\n', '', TEXT)\n",
    "            TEXT = re.sub('[’“”…]', '', TEXT)\n",
    "\n",
    "        #removing emoji: \n",
    "            emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "            TEXT = emoji_pattern.sub(r'', TEXT)   \n",
    "\n",
    "        # removing short form: \n",
    "            TEXT=re.sub(\"isn't\",'is not',TEXT)\n",
    "            TEXT=re.sub(\"he's\",'he is',TEXT)\n",
    "            TEXT=re.sub(\"wasn't\",'was not',TEXT)\n",
    "            TEXT=re.sub(\"there's\",'there is',TEXT)\n",
    "            TEXT=re.sub(\"couldn't\",'could not',TEXT)\n",
    "            TEXT=re.sub(\"won't\",'will not',TEXT)\n",
    "            TEXT=re.sub(\"they're\",'they are',TEXT)\n",
    "            TEXT=re.sub(\"she's\",'she is',TEXT)\n",
    "            TEXT=re.sub(\"There's\",'there is',TEXT)\n",
    "            TEXT=re.sub(\"wouldn't\",'would not',TEXT)\n",
    "            TEXT=re.sub(\"haven't\",'have not',TEXT)\n",
    "            TEXT=re.sub(\"That's\",'That is',TEXT)\n",
    "            TEXT=re.sub(\"you've\",'you have',TEXT)\n",
    "            TEXT=re.sub(\"He's\",'He is',TEXT)\n",
    "            TEXT=re.sub(\"what's\",'what is',TEXT)\n",
    "            TEXT=re.sub(\"weren't\",'were not',TEXT)\n",
    "            TEXT=re.sub(\"we're\",'we are',TEXT)\n",
    "            TEXT=re.sub(\"hasn't\",'has not',TEXT)\n",
    "            TEXT=re.sub(\"you'd\",'you would',TEXT)\n",
    "            TEXT=re.sub(\"shouldn't\",'should not',TEXT)\n",
    "            TEXT=re.sub(\"let's\",'let us',TEXT)\n",
    "            TEXT=re.sub(\"they've\",'they have',TEXT)\n",
    "            TEXT=re.sub(\"You'll\",'You will',TEXT)\n",
    "            TEXT=re.sub(\"i'm\",'i am',TEXT)\n",
    "            TEXT=re.sub(\"we've\",'we have',TEXT)\n",
    "            TEXT=re.sub(\"it's\",'it is',TEXT)\n",
    "        \n",
    "            TEXT=re.sub(\"don't\",'do not',TEXT)\n",
    "            TEXT=re.sub(\"that´s\",'that is',TEXT)\n",
    "            TEXT=re.sub(\"I´m\",'I am',TEXT)\n",
    "            TEXT=re.sub(\"it’s\",'it is',TEXT)\n",
    "            TEXT=re.sub(\"she´s\",'she is',TEXT)\n",
    "            TEXT=re.sub(\"he’s'\",'he is',TEXT)\n",
    "            TEXT=re.sub('I’m','I am',TEXT)\n",
    "            TEXT=re.sub('I’d','I did',TEXT)\n",
    "            TEXT=re.sub(\"he’s'\",'he is',TEXT)\n",
    "            TEXT=re.sub('there’s','there is',TEXT)\n",
    "    \n",
    "     \n",
    "        return TEXT\n",
    "       \n",
    "\n",
    "    def remove_stop_words(self, text: str) -> str:\n",
    "        return ' '.join([word for word in text.split() if word not in self.stop_words])\n",
    "\n",
    "    def remove_freq_words(self, text: str, freq_words: Iterable[str] = None) -> str:\n",
    "        if freq_words is None:\n",
    "            freq_words = []  # Set a default value (empty list) for freq_words\n",
    "        return ' '.join([word for word in text.split() if word not in freq_words])\n",
    "    def lemmatize_text(self, text: str) -> str:\n",
    "        return ' '.join([self.wordnet_lem.lemmatize(word) for word in text.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ea3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.count_vectorizer = CountVectorizer(ngram_range=(1, 2), binary=False)\n",
    "        self.tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "\n",
    "    def fit_transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        X_count = self.count_vectorizer.fit_transform(X)\n",
    "        X_tfidf = self.tfidf_transformer.fit_transform(X_count)\n",
    "        return X_tfidf\n",
    "\n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        X_count = self.count_vectorizer.transform(X)\n",
    "        X_tfidf = self.tfidf_transformer.transform(X_count)\n",
    "        return X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9a0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier:\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression(penalty='l2', solver='liblinear', multi_class='auto', C=100)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def score(self, X: pd.DataFrame, y_true: pd.Series, scoring_func, average=None) -> float:\n",
    "        y_pred = self.predict(X)\n",
    "        return scoring_func(y_true, y_pred, average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7890e58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9164\n",
      "Confusion Matrix:\n",
      " [[6383   46   25]\n",
      " [ 169 3379  308]\n",
      " [  90  375 3289]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "\n",
    "    preprocessor = TextPreprocessor()\n",
    "    train_data = preprocessor.preprocess_dataframe(train_data, 'TEXT')\n",
    "    test_data = preprocessor.preprocess_dataframe(test_data, 'TEXT')\n",
    "\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    X = feature_extractor.fit_transform(train_data['TEXT'])\n",
    "    y = train_data['LABEL']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    sentiment_classifier = SentimentClassifier()\n",
    "    sentiment_classifier.fit(X_train, y_train)\n",
    "\n",
    "    f1 = sentiment_classifier.score(X_test, y_test, f1_score, average='macro')\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, sentiment_classifier.predict(X_test))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    X_submission = feature_extractor.transform(test_data['TEXT'])\n",
    "    predicted_labels = sentiment_classifier.predict(X_submission)\n",
    "    test_data['LABEL'] = predicted_labels\n",
    "    test_data = test_data.drop('TEXT', axis=1)\n",
    "    test_data.to_csv('C:/Users/sweth/Downloads/submission.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25b9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
